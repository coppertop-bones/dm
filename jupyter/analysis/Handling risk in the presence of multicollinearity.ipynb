{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT'S THE PROBLEM WE ARE TRYING TO SOLVE?\n",
    "\n",
    "We wish to quantify the risk of a portfolio where the timeseries of relevant asset values exhibits multicollinearity (look it up, e.g. [on wikipedia](https://en.wikipedia.org/wiki/Multicollinearity)).\n",
    "\n",
    "In this example our domain is US yield to maturities and out portfolio is a position in the 7 year asset. We'll assume that the cheapest hedging instruments are at the 2y, 5y, 10y and 30y points.\n",
    "\n",
    "The following code uses coppertop-bones.\n",
    "\n",
    "<br>\n",
    "\n",
    "*Concepts to become familiar with...* \\\n",
    "Risk - what is it? \\\n",
    "Risk-factors \\\n",
    "Model, modelling, model risk, modelling risk, implementation risk. \\\n",
    "Supervised vs unsupervised learning. What are the implications of each? \\\n",
    "Why are residuals important? \\\n",
    "Over-fitting, data splitting (& carve-out?), cross-validation, statistical tests.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import datetime, numpy as np, plotnine, statsmodels.api as sm, scipy.stats, sys\n",
    "from numpy import linalg\n",
    "\n",
    "from coppertop.pipe import *\n",
    "\n",
    "from dm.core.types import btup, bframe, date, num, count as tCount, pylist, T1, txt, offset, pytuple, bstruct, \\\n",
    "    index, void, matrix, N\n",
    "from dm.ols import OLSResult\n",
    "from dm.jupyter import P9\n",
    "from dm.panda import panda\n",
    "\n",
    "from bones.core.sentinels import Void\n",
    "from bones.lang.structs import tvarray\n",
    "\n",
    "import dm.linalg.core, dm.pp, dm.panda, dm.jupyter, dm.plot\n",
    "from broot import *\n",
    "from broot import dm, seaborn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from bones.lang.metatypes import BTNom\n",
    "from bones.lang.structs import tv\n",
    "\n",
    "p9 = BTNom.ensure('p9').setCoercer(tv)\n",
    "\n",
    "@coppertop\n",
    "def PP(p: p9) -> p9:\n",
    "    fig = p._v.draw(show=True)\n",
    "    return p\n",
    "\n",
    "array_ = (N**num)&tvarray\n",
    "DATA_PATH = \"./data\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "@coppertop\n",
    "def residualsPlot(res:array_) -> P9:\n",
    "    # res = res >> to >> (vec&tvarray)\n",
    "    df = panda({'i':range(res >> count), 'res':res})\n",
    "    answer = (plotnine.ggplot(df, plotnine.aes(x='i', y='res')) + plotnine.geom_point())\n",
    "    answer = answer | P9\n",
    "        # + p9.geom_line(color='lightgrey', size=0.25)\n",
    "    return answer\n",
    "\n",
    "\n",
    "@coppertop\n",
    "def showStats(Y, X):\n",
    "    return showStats(Y, X, dict(addIntercept=False))\n",
    "\n",
    "@coppertop\n",
    "def showStats(Y, X, options):\n",
    "    lm = dm.stats.ols(Y, X, options)\n",
    "    f'beta:    {(lm >> betaHat >> round(_, 3)).reshape(lm.K)}' >> PP #      resDoF: {lm.resDoF}' >> PP\n",
    "    f't-stats: {lm >> tStats >> round(_, 3)} {lm >> tVals >> round(_, 3)}' >> PP\n",
    "    f'F-stat:  {np.round(lm >> fStat, 1)} (5% is > {np.round(lm >> fCrit(_,0.05), 1)}), {np.round(lm >> fVal, 4)}    R2: {np.round(lm >> r2, 3)}   predR2: {np.round(dm.stats.predictedR2(Y, X, options), 3)}' >> PP\n",
    "    return lm\n",
    "\n",
    "@coppertop\n",
    "def showResStats(x):\n",
    "    av = np.mean(res); st = np.std(res); sk = scipy.stats.skew(res); ku = scipy.stats.kurtosis(res)\n",
    "    des = scipy.stats.describe(res)\n",
    "    skt = scipy.stats.skewtest(res)\n",
    "    kut = scipy.stats.kurtosistest(res)\n",
    "    sk = des.skewness if skt.pvalue < 0.05 else 0.0\n",
    "    ku = des.kurtosis if kut.pvalue < 0.05 else 0.0\n",
    "    f\"res - mean: {format(des.mean, '.4f')}  std: {format(np.sqrt(des.variance), '.4f')}  skew: {format(sk, '.4f')}  kurt: {format(ku, '.4f')}\" >> PP\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "renames = {\n",
    "    'Date':'date', '1 Mo':'_1m', '2 Mo':'_2m', '3 Mo':'_3m', '6 Mo':'_6m', '1 Yr':'_1y', '2 Yr':'_2y', \n",
    "    '3 Yr':'_3y', '5 Yr':'_5y', '7 Yr':'_7y', '10 Yr':'_10y', '20 Yr':'_20y', '30 Yr':'_30y',\n",
    "}\n",
    "\n",
    "conversions = dict(\n",
    "    date=to(_, (N**date)&tvarray, 'MM/DD/YY'), _1m=to(_,array_), _2m=to(_,array_), _3m=to(_,array_),\n",
    "    _6m=to(_,array_), _1y=to(_,array_), _2y=to(_,array_), _3y=to(_,array_), _5y=to(_,array_),\n",
    "    _7y=to(_,array_), _10y=to(_,array_), _20y=to(_,array_), _30y=to(_,array_),\n",
    ")\n",
    "\n",
    "path = DATA_PATH\n",
    "filename = 'us yields.csv'\n",
    "ytms = (path + '/' + filename) >> dm.frame.fromCsv(_, renames, conversions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select three months worth and calc diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "source": [
    "d1 = '2021.01.01' >> to(_, _, 'YYYY.MM.DD') >> date\n",
    "d2 = '2021.04.01' >> to(_, _, 'YYYY.MM.DD') >> date\n",
    "\n",
    "subset = ytms >> select >> (lambda r: d1 <= r.date and r.date < d2)\n",
    "usDiffs = subset >> keys >> drop >> 'date' \\\n",
    "    >> inject(_, bframe(), _) >> (lambda p, name:\n",
    "        p >> atPut >> name >> (subset >> at >> name >> diff)\n",
    "    )\n",
    "\n",
    "# TODO something like the following is clearer? \n",
    "# usDiffs = subset >> drop >> 'date' >> bycols >> collect >> lambda(name, col: col >> vec_.diff)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### OLS\n",
    "\n",
    "Let's try the following models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "ytm2y = usDiffs >> take >> '_2y' >> takePanel\n",
    "ytm5y = usDiffs >> take >> '_5y' >> takePanel\n",
    "ytm7y = usDiffs >> take >> '_7y' >> takePanel\n",
    "ytm10y = usDiffs >> take >> '_10y' >> takePanel\n",
    "ytm30y = usDiffs >> take >> '_30y' >> takePanel\n",
    "ytm2y5y =  usDiffs >> take >> ['_2y', '_5y']  >> takePanel\n",
    "ytm5y10y =  usDiffs >> take >> ['_5y', '_10y']  >> takePanel\n",
    "ytm10y30y = usDiffs >> take >> ['_10y', '_30y']  >> takePanel\n",
    "ytm10y20y = usDiffs >> take >> ['_10y', '_20y']  >> takePanel\n",
    "ytm2y5y10y = usDiffs >> take >> ['_2y', '_5y', '_10y']  >> takePanel\n",
    "ytm3y5y10y = usDiffs >> take >> ['_3y', '_5y', '_10y']  >> takePanel     # maybe 3y is better than 2y?\n",
    "ytm5y10y30y = usDiffs >> take >> ['_5y', '_10y', '_30y']  >> takePanel\n",
    "ytm5y10y20y = usDiffs >> take >> ['_5y', '_10y', '_20y']  >> takePanel   # maybe 20y is better than 30y?\n",
    "ytm2y5y10y30y = usDiffs >> take >> ['_2y', '_5y', '_10y', '_30y']  >> takePanel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "\"\\n1) 7y ~ 2y\" >> PP\n",
    "showStats(ytm7y, ytm2y)\n",
    "\n",
    "\"\\n2) 7y ~ 5y\" >> PP\n",
    "showStats(ytm7y, ytm5y)\n",
    "\n",
    "\"\\n3) 7y ~ 10y\" >> PP\n",
    "showStats(ytm7y, ytm10y)\n",
    "\n",
    "\"\\n4) 7y ~ 30y\" >> PP\n",
    "showStats(ytm7y, ytm30y)\n",
    "\n",
    "\"\\n5) 7y ~ 2y,5y\" >> PP\n",
    "showStats(ytm7y, ytm2y5y)\n",
    "\n",
    "\"\\n6) 7y ~ 5y,10y\" >> PP\n",
    "showStats(ytm7y, ytm5y10y)\n",
    "\n",
    "\"\\n7) 7y ~ 10y,30y\" >> PP\n",
    "showStats(ytm7y, ytm10y30y)\n",
    "\n",
    "\"\\n7b) 7y ~ 10y,20y\" >> PP\n",
    "showStats(ytm7y, ytm10y20y)\n",
    "\n",
    "\"\\n8) 7y ~ 2y,5y,10y\" >> PP\n",
    "showStats(ytm7y, ytm2y5y10y)\n",
    "\n",
    "\"\\n9) 7y ~ 3y,5y,10y\" >> PP\n",
    "showStats(ytm7y, ytm3y5y10y)\n",
    "\n",
    "\"\\n10) 7y ~ 5y,10y,30y\" >> PP\n",
    "showStats(ytm7y, ytm5y10y30y)\n",
    "\n",
    "\"\\n11) 7y ~ 5y,10y,20y\" >> PP\n",
    "showStats(ytm7y, ytm5y10y20y)\n",
    "\n",
    "\"\\n12) 7y ~ 2y,5y,10y,30y\" >> PP\n",
    "showStats(ytm7y, ytm2y5y10y30y);"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Analysis / conclusions\n",
    "\n",
    "Reject models 5, 7b, 8, 9, 10, 11 & 12 because the p-stat is not significant.\n",
    "\n",
    "![p-stats](xkcd-p-stats.png)\n",
    "\n",
    "7 needs some further investigation (1, 2, 3, 4 and 6 are as anticipated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "\"7) 7y ~ 10y,30y\" >> PP\n",
    "showStats(ytm7y, ytm10y30y);"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "\"7) 7y ~ 10y,30y\" >> PP\n",
    "showStats(ytm7y, ytm10y30y, dict(addIntercept=True));"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "# sm.OLS(ytm7y, sm.add_constant(ytm10y30y)).fit().summary() >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "\"6) 7y ~ 5y,10y\" >> PP\n",
    "showStats(ytm7y, ytm5y10y);"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "\"6) 7y ~ 5y,10y\" >> PP\n",
    "showStats(ytm7y, ytm5y10y, dict(addIntercept=True));dict(addIntercept=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In both cases clearly we shouldn't be adding an intercept.\n",
    "\n",
    "We would select 6) in preference to 7) due to the F-test result and R-squared but also 7)'s betaHat is a long short hedge that implies a sort of mechanical relationship (like rates are pivotting like a see-saw on a 10y pivot) that is hard to believe in.\n",
    "\n",
    "We should also investigate some constrained regressions, e.g. Least Angle Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "lm = showStats(ytm7y, ytm2y)\n",
    "res = lm >> residuals(_, ytm7y, ytm2y)\n",
    "res >> showResStats\n",
    "res >> dm.plot.qq\n",
    "res >> residualsPlot >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "sm.OLS(ytm7y, sm.add_constant(ytm2y)).fit().summary() >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "lm = showStats(ytm7y, ytm5y)\n",
    "res = lm >> residuals(_, ytm7y, ytm5y)\n",
    "res >> showResStats\n",
    "res >> dm.plot.qq\n",
    "res >> residualsPlot >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "sm.OLS(ytm7y, sm.add_constant(ytm5y)).fit().summary() >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "lm = showStats(ytm7y, ytm10y)\n",
    "res = lm >> residuals(_, ytm7y, ytm10y)\n",
    "res >> showResStats\n",
    "res >> dm.plot.qq\n",
    "res >> residualsPlot >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "sm.OLS(ytm7y, sm.add_constant(ytm10y)).fit().summary() >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a bit of a concern, we're getting some quantisation that we're not seeing in the regression against 5y. And the distribution is a bit squished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "lm = showStats(ytm7y, ytm30y)\n",
    "res = lm >> residuals(_, ytm7y, ytm30y)\n",
    "res >> showResStats\n",
    "res >> dm.plot.qq\n",
    "res >> residualsPlot >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "sm.OLS(ytm7y, sm.add_constant(ytm30y)).fit().summary() >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "lm = showStats(ytm7y, ytm5y10y)\n",
    "res = lm >> residuals(_, ytm7y, ytm5y10y)\n",
    "res >> showResStats\n",
    "res >> dm.plot.qq\n",
    "res >> residualsPlot >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "sm.OLS(ytm7y, sm.add_constant(ytm5y10y)).fit().summary() >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB: Using SVD to perform PCA\n",
    "\n",
    "References - https://www.quora.com/What-is-the-difference-between-PCA-and-SVD and https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\n",
    "\n",
    "For a n x m panel matrix $P$, with n observations and m variables, let $C$ be the m x m covariance matrix $P ^ T P$. First let's consider the eigen decomposition of the covariance matrix - the resulting eigenvectors are the \"principal components\" and the eigen values the variance each eigen vector contributes to the overall variance.\n",
    "\n",
    "A (nonzero) vector $v$ of dimension m is an eigenvector of a square m × m matrix $C$ if it satisfies a linear equation of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C v = \\lambda v\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let $C$ be a square m × m matrix with m linearly independent eigenvectors $v_i$ (where i = 1, ..., m). Then $C$ can be factorized as\n",
    "\n",
    "\n",
    "$$\n",
    "C = V \\Lambda  V^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "where $V$ is the square m × m matrix whose ith column is the eigenvector $v_i$ of $C$, and $\\Lambda$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, $\\Lambda_ii = \\lambda_i$.\n",
    "\n",
    "$$\n",
    "C V = V \\Lambda\n",
    "$$\n",
    "\n",
    "\n",
    "Note that because  $P ^ T P$  is symmetric, $V$ is _**orthogonal**_ and the eigenvalues are all real-valued. This generates our \"principal components\" $V$ onto which we can project $P$ to get a set of uncorrelated variables:\n",
    "\n",
    "$$P' = PV$$\n",
    "\n",
    "\n",
    "Often we want $P'$ to be normalized, so dividing through by standard deviation, we get:\n",
    "\n",
    "$$U = P' \\Lambda ^{−1/2}$$ \n",
    "\n",
    "Substitute $PV$ for $P'$ and rearrange:\n",
    "\n",
    "$$U = PV\\Lambda^{-1/2}$$\n",
    "\n",
    "$$P = U \\Lambda^{1/2} V^T$$ \n",
    "\n",
    "i.e. the Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### PCA factors for the whole curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "panel = usDiffs >> drop >> ['_1m', '_2m', '_3m', '_6m'] >> takePanel\n",
    "vor, s = dm.linalg.pca(panel)\n",
    "s >> PP\n",
    "xs = [1, 2, 3, 5, 7, 10, 20, 30]\n",
    "plt.plot(xs, vor[:,0], 'red')\n",
    "plt.plot(xs, vor[:,1], 'orange')\n",
    "plt.plot(xs, vor[:,2], 'green')\n",
    "plt.plot(xs, vor[:,3]);"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### PCA factors for the hedge points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "panel = usDiffs >> take >> ['_2y', '_5y', '_10y', '_30y']  >> takePanel\n",
    "vor, s = dm.linalg.pca(panel)\n",
    "s >> PP\n",
    "xs = [2, 5, 10, 30]\n",
    "plt.plot(xs, vor[:,0], 'red')\n",
    "plt.plot(xs, vor[:,1], 'orange')\n",
    "plt.plot(xs, vor[:,2], 'green')\n",
    "plt.plot(xs, vor[:,3]);"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor 1 seems fairly consistent, factors 2 and 3 flip signs. showing that the set of selected inputs impacts the analysis (even with the orientation algo)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
