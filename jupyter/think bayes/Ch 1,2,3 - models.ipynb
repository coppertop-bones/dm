{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think Bayes solutions using coppertop\n",
    "\n",
    "In this notebook we use coppertop to work through some of the examples, problems and excerises from Allen B. Downey's book _Think Bayes_ - a copy of which can be found [here](https://github.com/coppertop-bones/dm/blob/main/jupyter/think%20bayes/thinkbayes.pdf).\n",
    "\n",
    "In the document:\n",
    "\n",
    "_Permission is granted to copy, distribute, and/or modify this document\n",
    "under the terms of the Creative Commons Attribution-NonCommercial\n",
    "3.0 Unported License, which is available at http://creativecommons.org/\n",
    "licenses/by-nc/3.0/_\n",
    "\n",
    "See [coppertop-bones/README.md](https://github.com/coppertop-bones/coppertop) for notes on usage of coppertop.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### imports and enum definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, numpy as np, enum"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "source": [
    "from coppertop.pipe import *\n",
    "\n",
    "from dm.core.types import dstruct, dmap, pytuple, pylist, T, T1, T2, T3, num, txt, py\n",
    "from dm.pmf import L, PMF, CMF, formatPmf\n",
    "import dm.pp                          # load pp functions\n",
    "from _ import *                       # import all loaded uber functions\n",
    "\n",
    "class E(enum.IntEnum):\n",
    "    A = enum.auto()\n",
    "    B = enum.auto()\n",
    "    C = enum.auto()\n",
    "    J1 = enum.auto()\n",
    "    J2 = enum.auto()\n",
    "    D4 = enum.auto()\n",
    "    D6 = enum.auto()\n",
    "    D8 = enum.auto()\n",
    "    D12 = enum.auto()\n",
    "    D20 = enum.auto()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "A = E.A\n",
    "B = E.B\n",
    "C = E.C\n",
    "J1 = E.J1\n",
    "J2 = E.J2\n",
    "D4 = E.D4\n",
    "D6 = E.D6\n",
    "D8 = E.D8\n",
    "D12 = E.D12\n",
    "D20 = E.D20"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.3 The cookie problem\n",
    "\n",
    "We’ll get to Bayes’s theorem soon, but I want to motivate it with an example\n",
    "called the cookie problem. Suppose there are two bowls of cookies. Bowl 1\n",
    "contains 30 vanilla cookies and 10 chocolate cookies. Bowl 2 contains 20 of\n",
    "each.\n",
    "\n",
    "Now suppose you choose one of the bowls at random and, without looking,\n",
    "select a cookie at random. The cookie is vanilla. What is the probability that\n",
    "it came from Bowl 1?\n",
    "\n",
    "This is a conditional probability; we want $\\mathbf{P}\\left(Bowl1 \\mathbin{\\vert} vanilla\\right)$, but it is not\n",
    "obvious how to compute it. If I asked a different question—the probability\n",
    "of a vanilla cookie given Bowl 1—it would be easy:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}( vanilla\\mathbin{\\vert}Bowl1) = 3/4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Sadly, $\\mathbf{P}\\left(A\\mathbin{\\vert}B\\right)$ is not the same as $\\mathbf{P}\\left(B\\mathbin{\\vert}A\\right)$, but \n",
    "there is a way to get from one to the other: Bayes’s theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Bayes Refresher\n",
    "See _[\"An Essay towards solving a Problem in the Doctrine of Chances\"](https://github.com/coppertop-bones/dm/blob/main/jupyter/think%20bayes/article.pdf)_.\n",
    "\n",
    "from PROP 3\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}\\left(B \\cap A\\right) = \\mathbf{P}\\left(B\\mathbin{\\vert}A\\right)\\cdot \\mathbf{P}\\left(A\\right)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and obviously\n",
    "$$\\mathbf{P}(A \\cap B) = \\mathbf{P}(B \\cap A)$$\n",
    "\n",
    "so\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}( A\\mathbin{\\vert}B) \\cdot \\mathbf{P}(B)=\\mathbf{P}(B\\mathbin{\\vert}A)\\cdot \\mathbf{P}(A)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "aka\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}( hypothesis\\mathbin{\\vert}data) \\cdot \\mathbf{P}(data)=\\mathbf{P}(data\\mathbin{\\vert}hypothesis)\\cdot \\mathbf{P}(hypothesis)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Comtemporaneous (look it up) version, i.e. after some data is known\n",
    "$$\n",
    "\\begin{align}\n",
    "posterior =likelihood\\cdot prior \\cdot constant\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.6 The M&M Problem\n",
    "\n",
    "M&M’s are small candy-coated chocolates that come in a variety of colors.\n",
    "Mars, Inc., which makes M&M’s, changes the mixture of colors from time\n",
    "to time.\n",
    "\n",
    "In 1995, they introduced blue M&M’s. Before then, the color mix in a bag\n",
    "of plain M&M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10%\n",
    "Orange, 10% Tan. Afterward it was 24% Blue , 20% Green, 16% Orange,\n",
    "14% Yellow, 13% Red, 13% Brown.\n",
    "\n",
    "Suppose a friend of mine has two bags of M&M’s, and he tells me that one\n",
    "is from 1994 and one from 1996. He won’t tell me which is which, but he\n",
    "gives me one M&M from each bag. One is yellow and one is green. What is\n",
    "the probability that the yellow one came from the 1994 bag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "source": [
    "bag1994 = dstruct(Brown=30, Yellow=20, Red=20, Green=10, Orange=10, Tan=10)\n",
    "bag1996 = dstruct(Brown=13, Yellow=14, Red=13, Green=20, Orange=16, Blue=24)\n",
    "[bag1994, bag1996] >> collect >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypA -> yellow is from 1994, green is from 1996\\\n",
    "hypB -> green is from 1994, yellow is from 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "source": [
    "prior = PMF({A:0.5, B:0.5}) >> PP\n",
    "\n",
    "likelihood = L({\n",
    "    A: bag1994.Yellow * bag1996.Green, \n",
    "    B: bag1994.Green * bag1996.Yellow\n",
    "}) >> PP\n",
    "\n",
    "post = prior >> pmfMul >> likelihood >> normalise\n",
    "post >> PP\n",
    "\n",
    "20/27"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.7 The Monty Hall problem\n",
    "\n",
    "Monty Hall was the original host of the game show Let’s Make a Deal. The\n",
    "Monty Hall problem is based on one of the regular games on the show. If\n",
    "you are on the show, here’s what happens:\n",
    "\n",
    "• Monty shows you three closed doors and tells you that there is a prize\n",
    "behind each door: one prize is a car, the other two are less valuable\n",
    "prizes like peanut butter and fake finger nails. The prizes are arranged\n",
    "at random.\n",
    "\n",
    "• The object of the game is to guess which door has the car. If you guess\n",
    "right, you get to keep the car.\n",
    "\n",
    "• You pick a door, which we will call Door A. We’ll call the other doors\n",
    "B and C.\n",
    "\n",
    "• Before opening the door you chose, Monty increases the suspense by\n",
    "opening either Door B or C, whichever does not have the car. (If the\n",
    "car is actually behind Door A, Monty can safely open B or C, so he\n",
    "chooses one at random.)\n",
    "\n",
    "• Then Monty offers you the option to stick with your original choice or\n",
    "switch to the one remaining unopened door.\n",
    "\n",
    "The question is, should you “stick” or “switch” or does it make no difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minor reframe**\n",
    "\n",
    "Let A be the door we initially choose at random \\\n",
    "Let B be the door Monty selects to show us to be without a car \\\n",
    "Let C be the other door we can choose after the fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "source": [
    "prior = PMF({A:1, B:1, C:1}) >> PP\n",
    "likelihood = L({ # i.e. likelihood of monty opening B given that the car is behind each, i.e. p(data|hyp)\n",
    "    A: 0.5,      # prob of opening B if behind A - he can choose at random so 50:50\n",
    "    B: 0,        # prob of opening B if behind B - Monty can't open B else he'd reveal the car, so cannot open B => 0%\n",
    "    C: 1,        # prob of opening B if behind C - Monty can't open C else he'd reveal the car, so must open B => 100%\n",
    "})\n",
    "posterior = prior >> pmfMul >> likelihood >> normalise\n",
    "posterior >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 1.8 Discussion\n",
    "If the Monty Hall problem is your idea of fun, I have collected a number of similar problems in an article called “All your Bayes are belong to us,” which you can read at http://allendowney.blogspot.com/2011/10/ all-your-bayes-are-belong-to-us.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2.8 Exercises\n",
    "\n",
    "**Exercise 2.1.** In Section 2.3 I said that the solution to the cookie problem generalizes to the case where we draw multiple cookies with replacement.\n",
    "\n",
    "But in the more likely scenario where we eat the cookies we draw, the likelihood of each draw depends on the previous draws.\n",
    "\n",
    "Modify the solution in this chapter to handle selection without replacement. Hint: add instance variables to Cookie to represent the hypothetical state of the bowls, and modify Likelihood accordingly. You might want to define a Bowl object.\n",
    "\n",
    "<br> \n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "Select a jar, eat some cookies telling me the flavours. This is the one we implement.\n",
    "\n",
    "Selecting a jar for each cookie would involve keeping tracking of possible combinations of jars and cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "source": [
    "@coppertop\n",
    "def jarLikelihood(jarsStates, flavour) -> L:\n",
    "    return jarsStates >> collect >> (lambda j: (j.tag, j >> atSlot >> flavour)) >> to >> L\n",
    "\n",
    "@coppertop\n",
    "def updateJarModel(jarsStateAndPrior, flavour):\n",
    "    jarsState, prior = jarsStateAndPrior\n",
    "    posterior = prior >> pmfMul >> jarLikelihood(jarsState, flavour) >> normalise\n",
    "    jarsState = jarsState >> collect >> (lambda s: s >> atPut >> flavour >> max(((s >> at >> flavour) - 1, 0)))\n",
    "    f'Took: {flavour},  posterior: {posterior >> normalise >> formatPmf},  newState: {jarsState}' >> PP\n",
    "    return (jarsState, posterior)\n",
    "\n",
    "modelState = [dstruct(V=30, C=10, tag=J1), dstruct(V=20, C=20, tag=J2)]\n",
    "\n",
    "['C', 'V'] >> inject(_, (modelState, PMF({J1:0.5, J2:0.5})), _) >> updateJarModel;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "source": [
    "['V', 'C'] >> inject(_, (modelState, PMF({J1:0.5, J2:0.5})), _) >> updateJarModel;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "source": [
    "['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] \\\n",
    "  >> inject(_, (modelState, PMF({J1:0.5, J2:0.5})), _) >> updateJarModel;"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.1 The dice problem\n",
    "\n",
    "Suppose I have a box of dice that contains a 4-sided die, a 6-sided die, an 8-sided die, a 12-sided die, and a 20-sided die. If you have ever played Dungeons & Dragons, you know what I am talking about.\n",
    "\n",
    "Suppose I select a die from the box at random, roll it, and get a 6. What is the probability that I rolled each die?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "source": [
    "@coppertop\n",
    "def diceLikelihood(ds, number):\n",
    "    return ds >> collect >> (lambda d: (d.tag, d >> atOr >> number >> 0.0)) >> to >> L\n",
    "\n",
    "d4 = (sequence(1, 4) >> uniform)(tag=D4)\n",
    "d6 = (sequence(1, 6) >> uniform)(tag=D6)\n",
    "d8 = (sequence(1, 8) >> uniform)(tag=D8)\n",
    "d12 = (sequence(1, 12) >> uniform)(tag=D12)\n",
    "d20 = (sequence(1, 20) >> uniform)(tag=D20)\n",
    "\n",
    "rolled = 6\n",
    "\n",
    "model = [d4, d6, d8, d12, d20]\n",
    "PMF({D4:1, D6:1, D8:1, D12:1, D20:1}) >> PP >> pmfMul >> (diceLikelihood(model, rolled) >> PP) >> normalise >> PP;"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "source": [
    "@coppertop\n",
    "def diceUpdate(prior, model, data):\n",
    "    posterior = prior >> pmfMul >> diceLikelihood(model, data) >> normalise\n",
    "    return posterior >> PP\n",
    "\n",
    "[6, 6, 8, 7, 7, 5, 4] >> inject(_, PMF({D4:1, D6:1, D8:1, D12:1, D20:1}), _) >> diceUpdate(_, model, _);"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.2 The locomotive problem\n",
    "\n",
    "A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "The railroad has more than 1000 locomotives \\\n",
    "If the railroad has N locomotives then the chance of seeing a particular locomotive is uniformly distributed with P = 1/N \\\n",
    "See http://en.wikipedia.org/wiki/Minimum_mean_square_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "source": [
    "prior = sequence(1, 1000) >> uniform\n",
    "data = 60\n",
    "\n",
    "@coppertop\n",
    "def railroadLikelihood(N, ob):\n",
    "    return sequence(1, N) \\\n",
    "        >> collect >> (lambda hyp:  (hyp, 0) if hyp < ob else (hyp, 1 / hyp) ) \\\n",
    "        >> to >> L\n",
    "\n",
    "likelihood = railroadLikelihood(1000, data)\n",
    "posterior = prior >> pmfMul >> likelihood >> normalise\n",
    "\n",
    "[posterior \\\n",
    "     >> values \\\n",
    "     >> max, posterior >> at >> 60, posterior >> at >> 59, \n",
    "     posterior \\\n",
    "     >> mean\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "source": [
    "### fig = plt.figure(figsize=(5, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(*(posterior >> toSteps));"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm... that's quite a heavy tail..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "source": [
    "[250, 500, 1000, 2000, 4000] >> collect >> (lambda N: \n",
    "    sequence(1, N) \n",
    "        >> uniform\n",
    "        >> pmfMul \n",
    "        >> railroadLikelihood(N, data)\n",
    "        >> normalise \n",
    "        >> mean\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.4 An alternative prior\n",
    "\n",
    "In fact, the distribution of company sizes tends to follow a power law, as Robert Axtell reports in Science (see http://www.sciencemag.org/content/293/5536/1818.full.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "source": [
    "@coppertop\n",
    "def powerLawPrior(n, alpha) -> PMF:\n",
    "    return sequence(1, 1000) >> collect >> (lambda hyp: (hyp, hyp**(-alpha))) >> to >> PMF\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(*(1000 >> powerLawPrior(_, 1) >> toSteps));"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3.5 Credible intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "source": [
    "observations = [30, 60, 90]\n",
    "Ns = [250, 500, 1000, 2000, 4000]\n",
    "\n",
    "Ns >> collect >> (lambda N: \n",
    "    powerLawPrior(N, 0.9)\n",
    "        >> inject(observations, _, _) >> (lambda prior, ob:\n",
    "            prior\n",
    "                >> pmfMul \n",
    "                >> railroadLikelihood(N, ob)\n",
    "                >> normalise \n",
    "        )\n",
    "        >> makeFn(lambda pmf: [pmf >> to >> CMF >> quantile(_,0.05), pmf >> mean, pmf >> to >> CMF >> quantile(_,0.95)])\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4.1 The Euro problem\n",
    "\n",
    "A statistical statement appeared in “The Guardian\" on Friday January 4, 2002:\n",
    "\n",
    "When spun on edge 250 times, a Belgian one-euro coin\n",
    "came up heads 140 times and tails 110. ‘It looks very\n",
    "suspicious to me,’ said Barry Blight, a statistics lecturer\n",
    "at the London School of Economics. ‘If the coin were\n",
    "unbiased, the chance of getting a result as extreme as\n",
    "that would be less than 7%.’\n",
    "\n",
    "But do these data give evidence that the coin is biased rather\n",
    "than fair?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "hypos = sequence(0, 100)\n",
    "\n",
    "@coppertop\n",
    "def euroLikelihoodFn(hyp, ob):\n",
    "    return (hyp, hyp / 100.0) if ob == 'H' else (hyp, 1 - hyp/100)\n",
    "\n",
    "@coppertop\n",
    "def euroUpdate(prior, ob):\n",
    "    like = hypos >> collect >> euroLikelihoodFn(_, ob) >> to >> L\n",
    "    return prior >> pmfMul >> like\n",
    "\n",
    "data = ['H'] * 140 + ['T'] * 110\n",
    "\n",
    "prior = hypos >> uniform     # p that number of heads == hypo\n",
    "\n",
    "posterior = data >> inject(_, prior, _) >> euroUpdate >> normalise\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(*(posterior >> toSteps));"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "[posterior >> to >> CMF >> quantile(_,0.05), posterior >> mean, posterior >> to >> CMF >> quantile(_,0.95)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "source": [
    "@coppertop\n",
    "def triangular(start, nUp, nDown) -> PMF:\n",
    "    d = {}\n",
    "    for x in range(0, 51):\n",
    "        d[x] = x\n",
    "    for x in range(51, 101):\n",
    "        d[x] = 100 - x\n",
    "    return d | PMF\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "source": [
    "# triangular(10, 10, 10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAL ERRORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\"hello\" >> toCmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Can't find toCmf(str) in:\n",
    "  toCmf(pmf:(adhoc + (t514 & _numEtAl & _PMF))) in dm.pmf\n",
    "```\n",
    "\n",
    "ideally\n",
    "\n",
    "```\n",
    "DispatchError: Can't find toCmf(str) in:\n",
    "  toCmf(pmf:PMF+adhoc) -> any    in dm.pmf[176]\n",
    "---------------------------------------------------------------------------\n",
    "Traceback (most recent call last)\n",
    "\n",
    "<ipython-input-26-8a7f9c734ab9> in <module>\n",
    "----> 1 \"hello\" >> toCmf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
